# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KhRlMQ7tcRYlwtRpx4qNWo7zLYjNsOvI
"""

import torch
from torch import nn, optim
from torchvision import transforms, datasets
import matplotlib.pyplot as plt

# Set random seed for reproducibility
torch.manual_seed(42)

# Hyperparameters
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 100
learning_rate = 0.0002
num_epochs = 81
num_test_samples = 32

# Data loading and preprocessing
transform = transforms.Compose([
    transforms.Resize(64),
    transforms.ToTensor(),
    transforms.Normalize((.5, .5, .5), (.5, .5, .5))
])

cifar10_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)
data_loader = torch.utils.data.DataLoader(cifar10_dataset, batch_size=batch_size, shuffle=True)

# Generator model
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.linear = nn.Linear(100, 1024*4*4)
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.linear(x)
        x = x.view(x.size(0), 1024, 4, 4)
        x = self.deconv(x)
        return x

# Discriminator model
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.2, inplace=True)
        )
        self.output = nn.Sequential(
            nn.Linear(1024*4*4, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), 1024*4*4)
        x = self.output(x)
        return x

# Initialize generator and discriminator
generator = Generator().to(device)
discriminator = Discriminator().to(device)

# Weight initialization
def weights_init(module):
    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):
        nn.init.normal_(module.weight.data, 0.0, 0.02)
    elif isinstance(module, nn.BatchNorm2d):
        nn.init.normal_(module.weight.data, 1.0, 0.02)
        nn.init.constant_(module.bias.data, 0)

from torchvision.utils import make_grid

generator.apply(weights_init)
discriminator.apply(weights_init)

# Loss function
criterion = nn.BCELoss()

# Optimizers
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))

# Fixed noise for visualization
fixed_noise = torch.randn(num_test_samples, 100, device=device)

# Training loop
discriminator_losses = []
generator_losses = []

for epoch in range(num_epochs):
    # Initialize loss variables for this epoch
    epoch_discriminator_loss = 0
    epoch_generator_loss = 0

    for i, (real_images, _) in enumerate(data_loader):
        real_images = real_images.to(device)

        # Discriminator forward pass with real images
        real_labels = torch.ones(batch_size, 1, device=device)
        real_outputs = discriminator(real_images)
        d_loss_real = criterion(real_outputs, real_labels)

        # Generator forward pass and discriminator backward pass with fake images
        noise = torch.randn(batch_size, 100, device=device)
        fake_images = generator(noise)
        fake_labels = torch.zeros(batch_size, 1, device=device)
        fake_outputs = discriminator(fake_images.detach())
        d_loss_fake = criterion(fake_outputs, fake_labels)

        d_loss = d_loss_real + d_loss_fake

        discriminator.zero_grad()
        d_loss.backward()
        d_optimizer.step()

        # Generator forward pass and discriminator backward pass with fake images
        fake_outputs = discriminator(fake_images)
        g_loss = criterion(fake_outputs, real_labels)

        generator.zero_grad()
        g_loss.backward()
        g_optimizer.step()

        # Accumulate losses for this epoch
        epoch_discriminator_loss += d_loss.item()
        epoch_generator_loss += g_loss.item()

    # Average losses for this epoch
    epoch_discriminator_loss /= len(data_loader)
    epoch_generator_loss /= len(data_loader)

    discriminator_losses.append(epoch_discriminator_loss)
    generator_losses.append(epoch_generator_loss)

    # Print progress
    print(f"Epoch [{epoch+1}/{num_epochs}], Discriminator Loss: {epoch_discriminator_loss:.4f}, Generator Loss: {epoch_generator_loss:.4f}")

    # Generate images
    with torch.no_grad():
        generated_images = generator(fixed_noise).detach().cpu()

    # Display generated images
    if (epoch+1) % 2 == 0 or epoch == 0:
        plt.figure(figsize=(8, 8))
        for j in range(num_test_samples):
            # Denormalize the image
            image = generated_images[j].permute(1, 2, 0)
            image = (image + 1) / 2  # Undo normalization

            plt.subplot(4, 8, j+1)
            plt.imshow(image)
            plt.axis('off')
        plt.show()

# Plotting the loss curve
plt.figure()
plt.plot(range(1, num_epochs+1), discriminator_losses, label="Discriminator Loss")
plt.plot(range(1, num_epochs+1), generator_losses, label="Generator Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.savefig("loss_curve.png")
plt.show()

# Set random seed for reproducibility
torch.manual_seed(42)

# Hyperparameters
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 100
learning_rate = 0.00002
num_epochs = 91
num_test_samples = 16

generator.apply(weights_init)
discriminator.apply(weights_init)

# Loss function
criterion = nn.BCELoss()

# Optimizers
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))

# Fixed noise for visualization
fixed_noise = torch.randn(num_test_samples, 100, device=device)

# Training loop
discriminator_losses = []
generator_losses = []

for epoch in range(num_epochs):
    # Initialize loss variables for this epoch
    epoch_discriminator_loss = 0
    epoch_generator_loss = 0

    for i, (real_images, _) in enumerate(data_loader):
        real_images = real_images.to(device)

        # Discriminator forward pass with real images
        real_labels = torch.ones(batch_size, 1, device=device)
        real_outputs = discriminator(real_images)
        d_loss_real = criterion(real_outputs, real_labels)

        # Generator forward pass and discriminator backward pass with fake images
        noise = torch.randn(batch_size, 100, device=device)
        fake_images = generator(noise)
        fake_labels = torch.zeros(batch_size, 1, device=device)
        fake_outputs = discriminator(fake_images.detach())
        d_loss_fake = criterion(fake_outputs, fake_labels)

        d_loss = d_loss_real + d_loss_fake

        discriminator.zero_grad()
        d_loss.backward()
        d_optimizer.step()

        # Generator forward pass and discriminator backward pass with fake images
        fake_outputs = discriminator(fake_images)
        g_loss = criterion(fake_outputs, real_labels)

        generator.zero_grad()
        g_loss.backward()
        g_optimizer.step()

        # Accumulate losses for this epoch
        epoch_discriminator_loss += d_loss.item()
        epoch_generator_loss += g_loss.item()

    # Average losses for this epoch
    epoch_discriminator_loss /= len(data_loader)
    epoch_generator_loss /= len(data_loader)

    discriminator_losses.append(epoch_discriminator_loss)
    generator_losses.append(epoch_generator_loss)

    # Print progress
    print(f"Epoch [{epoch+1}/{num_epochs}], Discriminator Loss: {epoch_discriminator_loss:.4f}, Generator Loss: {epoch_generator_loss:.4f}")

    # Generate images
    with torch.no_grad():
        generated_images = generator(fixed_noise).detach().cpu()

    # Display generated images
    if (epoch+1) % 2 == 0 or epoch == 0:
        plt.figure(figsize=(8, 8))
        for j in range(num_test_samples):
            # Denormalize the image
            image = generated_images[j].permute(1, 2, 0)
            image = (image + 1) / 2  # Undo normalization

            plt.subplot(4, 8, j+1)
            plt.imshow(image)
            plt.axis('off')
        plt.show()

# Plotting the loss curve
plt.figure()
plt.plot(range(1, num_epochs+1), discriminator_losses, label="Discriminator Loss")
plt.plot(range(1, num_epochs+1), generator_losses, label="Generator Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.savefig("loss_curve.png")
plt.show()

# Set random seed for reproducibility
torch.manual_seed(42)

# Hyperparameters
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 100
learning_rate = 0.000075
num_epochs = 200
num_test_samples = 16

# Data loading and preprocessing
transform = transforms.Compose([
    transforms.Resize(64),
    transforms.ToTensor(),
    transforms.Normalize((.5, .5, .5), (.5, .5, .5))
])

cifar10_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)
data_loader = torch.utils.data.DataLoader(cifar10_dataset, batch_size=batch_size, shuffle=True)

# Generator model
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.linear = nn.Linear(100, 1024*4*4)
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.linear(x)
        x = x.view(x.size(0), 1024, 4, 4)
        x = self.deconv(x)
        return x

# Discriminator model
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.2, inplace=True)
        )
        self.output = nn.Sequential(
            nn.Linear(1024*4*4, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), 1024*4*4)
        x = self.output(x)
        return x

# Initialize generator and discriminator
generator = Generator().to(device)
discriminator = Discriminator().to(device)

# Weight initialization
def weights_init(module):
    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):
        nn.init.normal_(module.weight.data, 0.0, 0.02)
    elif isinstance(module, nn.BatchNorm2d):
        nn.init.normal_(module.weight.data, 1.0, 0.02)
        nn.init.constant_(module.bias.data, 0)

generator.apply(weights_init)
discriminator.apply(weights_init)

# Loss function
criterion = nn.BCELoss()

# Optimizers
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))

# Fixed noise for visualization
fixed_noise = torch.randn(num_test_samples, 100, device=device)

# Training loop
discriminator_losses = []
generator_losses = []

for epoch in range(num_epochs):
    # Initialize loss variables for this epoch
    epoch_discriminator_loss = 0
    epoch_generator_loss = 0

    for i, (real_images, _) in enumerate(data_loader):
        real_images = real_images.to(device)

        # Discriminator forward pass with real images
        real_labels = torch.ones(batch_size, 1, device=device)
        real_outputs = discriminator(real_images)
        d_loss_real = criterion(real_outputs, real_labels)

        # Generator forward pass and discriminator backward pass with fake images
        noise = torch.randn(batch_size, 100, device=device)
        fake_images = generator(noise)
        fake_labels = torch.zeros(batch_size, 1, device=device)
        fake_outputs = discriminator(fake_images.detach())
        d_loss_fake = criterion(fake_outputs, fake_labels)

        d_loss = d_loss_real + d_loss_fake

        discriminator.zero_grad()
        d_loss.backward()
        d_optimizer.step()

        # Generator forward pass and discriminator backward pass with fake images
        fake_outputs = discriminator(fake_images)
        g_loss = criterion(fake_outputs, real_labels)

        generator.zero_grad()
        g_loss.backward()
        g_optimizer.step()

        # Accumulate losses for this epoch
        epoch_discriminator_loss += d_loss.item()
        epoch_generator_loss += g_loss.item()

    # Average losses for this epoch
    epoch_discriminator_loss /= len(data_loader)
    epoch_generator_loss /= len(data_loader)

    discriminator_losses.append(epoch_discriminator_loss)
    generator_losses.append(epoch_generator_loss)

    # Print progress
    print(f"Epoch [{epoch+1}/{num_epochs}], Discriminator Loss: {epoch_discriminator_loss:.4f}, Generator Loss: {epoch_generator_loss:.4f}")

    # Generate images
    with torch.no_grad():
        generated_images = generator(fixed_noise).detach().cpu()

    # Display generated images
    if (epoch+1) % 3 == 0 or epoch == 0:
        plt.figure(figsize=(8, 8))
        for j in range(num_test_samples):
            # Denormalize the image
            image = generated_images[j].permute(1, 2, 0)
            image = (image + 1) / 2  # Undo normalization

            plt.subplot(4, 8, j+1)
            plt.imshow(image)
            plt.axis('off')
        plt.show()

# Plotting the loss curve
plt.figure()
plt.plot(range(1, num_epochs+1), discriminator_losses, label="Discriminator Loss")
plt.plot(range(1, num_epochs+1), generator_losses, label="Generator Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.savefig("loss_curve.png")
plt.show()